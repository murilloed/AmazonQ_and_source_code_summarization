ğŸ§© Evaluation Framework â€” Metodologia CHT (Completeness, Helpfulness, Truthfulness)
ğŸ“˜ Contexto

Este documento descreve a metodologia CHT, desenvolvida e aplicada na pesquisa sobre uso do Amazon Q (AWS) para documentaÃ§Ã£o automatizada de cÃ³digo Java legado.
O framework fornece mÃ©tricas objetivas e reprodutÃ­veis para avaliar a qualidade de documentaÃ§Ã£o tÃ©cnica gerada por modelos de linguagem em sistemas crÃ­ticos, como o SASB.

âš™ï¸ Estrutura geral da metodologia

A metodologia estÃ¡ estruturada em trÃªs dimensÃµes complementares:

DimensÃ£o	Objetivo principal	Tipo de avaliaÃ§Ã£o
Completeness (C)	Medir o grau de cobertura do Javadoc	SintÃ¡tica / estrutural
Helpfulness (H)	Avaliar a clareza e utilidade prÃ¡tica para desenvolvedores	SemÃ¢ntica (LLM-as-a-Judge)
Truthfulness (T)	Garantir que as descriÃ§Ãµes correspondam fielmente ao cÃ³digo	Factual / lÃ³gica

Essas dimensÃµes formam o nÃºcleo do Evaluation Framework, que pode ser aplicado a qualquer pipeline de geraÃ§Ã£o de documentaÃ§Ã£o tÃ©cnica baseada em LLMs.

ğŸ”¬ 1. Completeness (Completude)
Como aplicar

Executar anÃ¡lise sintÃ¡tica automÃ¡tica (AST + expressÃµes regulares) para verificar se o Javadoc contÃ©m:

DescriÃ§Ã£o geral do mÃ©todo;

ParÃ¢metros (@param);

Retorno (@return);

ExceÃ§Ãµes (@throws);

Outras seÃ§Ãµes contextuais (quando aplicÃ¡veis).

Por que aplicar

Garante conformidade com os padrÃµes formais do JavaDoc e verifica se o Amazon Q cobre integralmente os campos obrigatÃ³rios esperados em projetos corporativos e pÃºblicos.

ğŸ’¡ 2. Helpfulness (Utilidade)
Como aplicar

Empregar um LLM avaliador (LLM-as-a-Judge) para atribuir notas (escala Likert 1â€“5) com base em:

Clareza e concisÃ£o;

Profundidade explicativa;

CoerÃªncia e completude das descriÃ§Ãµes dos parÃ¢metros;

OrientaÃ§Ãµes de uso e contexto prÃ¡tico.

Por que aplicar

Mede o valor real da documentaÃ§Ã£o para desenvolvedores humanos, fator crucial em sistemas legados e de difÃ­cil manutenÃ§Ã£o.

âœ… 3. Truthfulness (Veracidade)
Como aplicar

Checar se os elementos descritos na documentaÃ§Ã£o existem no cÃ³digo original:

ExtraÃ§Ã£o automÃ¡tica de entidades citadas (mÃ©todos, atributos, exceÃ§Ãµes);

ValidaÃ§Ã£o via grafo de dependÃªncias e Ã¡rvore de sintaxe abstrata (AST).

Por que aplicar

Evita â€œalucinaÃ§Ãµes de cÃ³digoâ€ â€” situaÃ§Ãµes em que o modelo descreve comportamentos ou entidades inexistentes.

ğŸ“Š 4. MÃ©tricas utilizadas
DimensÃ£o	Escala	TÃ©cnica de avaliaÃ§Ã£o	Indicador
C	0â€“1	AST + regex	Taxa de completude
H	1â€“5	LLM-as-a-Judge	MÃ©dia de utilidade
T	0â€“1	ComparaÃ§Ã£o factual via AST/grafo	Existence ratio
ğŸ“ˆ 5. Resultados obtidos (exemplo Amazon Q)
MÃ©trica	MÃ©dia Inicial	MÃ©dia Final	EvoluÃ§Ã£o (%)
Completeness (C)	0.52	0.83	+59%
Helpfulness (H)	2.44	4.12	+69%
Truthfulness (T)	1.00	1.00	0%

Esses valores demonstram melhoria consistente em completude e utilidade, sem perda de veracidade tÃ©cnica â€” indicando que o pipeline Amazon Q + ChatGPT Ã© eficaz para documentaÃ§Ã£o iterativa e confiÃ¡vel.

ğŸ§  6. ContribuiÃ§Ãµes cientÃ­ficas

Pipeline hÃ­brido LLMâ€“LLMâ€“Humano validado empiricamente (Amazon Q â†’ ChatGPT â†’ Amazon Q â†’ survey).

MÃ©tricas CHT operacionalizadas de forma automÃ¡tica e replicÃ¡vel.

Rigor metodolÃ³gico superior a mÃ©tricas de NLP tradicionais (BLEU/ROUGE).

Escalabilidade: aplicÃ¡vel a dezenas ou centenas de mÃ©todos sem intervenÃ§Ã£o manual direta.

AderÃªncia prÃ¡tica: alinhado a contextos reais de documentaÃ§Ã£o em software pÃºblico (ex.: SASB).

ğŸ§ª 7. Reprodutibilidade

Para replicar o experimento:

Entrada: mÃ©todo Java original (.java).

Etapas:
a. Enviar o cÃ³digo ao Amazon Q (geraÃ§Ã£o de documentaÃ§Ã£o).
b. Submeter a saÃ­da ao ChatGPT para avaliaÃ§Ã£o CHT.
c. Reenviar o feedback ao Amazon Q (refinamento).
d. Calcular mÃ©tricas automÃ¡ticas via script Python/AST.

SaÃ­das esperadas:

VersÃµes refinadas de documentaÃ§Ã£o (v1, v2, v3).

MÃ©tricas CHT agregadas em CSV/JSON.

ğŸ§© 8. AplicaÃ§Ãµes e limitaÃ§Ãµes

AplicaÃ§Ãµes:

AvaliaÃ§Ã£o de qualidade em pipelines LLM para engenharia de software.

Monitoramento automatizado de documentaÃ§Ã£o em sistemas legados.

Benchmark de LLMs em contextos tÃ©cnicos (Amazon Q, GPT, Claude, CodeWhisperer).

LimitaÃ§Ãµes:

DependÃªncia da qualidade do prompt few-shot;

Necessidade de contexto completo de classe (imports e anotaÃ§Ãµes);

DetecÃ§Ã£o parcial de inconsistÃªncias semÃ¢nticas profundas.

ğŸ“š 9. ReferÃªncias

Kruse, J. et al. (2024). Can Prompt Engineering Improve Code Documentation? ACM/ICSE.

Guo, T. et al. (2024). Q-Tuning for Developer Assistance. IEEE Software.

Sultan, M. et al. (2024). SCOT: Structured Code-Oriented Training for LLMs. Springer AI Review.

Hsieh, Y. et al. (2023). Tool Documentation Pipelines in Practice. Elsevier Journal of Software Systems.

ğŸ§¾ LicenÃ§a e citaÃ§Ã£o

Este material faz parte do projeto â€œQualificaÃ§Ã£o Murillo â€” UnB / Amazon Q Researchâ€
Sob licenÃ§a MIT para fins de pesquisa e reprodutibilidade.

Â© 2025 â€” Grupo de Pesquisa em InteligÃªncia Artificial Aplicada (UnB)
